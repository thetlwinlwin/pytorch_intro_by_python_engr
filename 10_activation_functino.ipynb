{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Activation functions apply a non-linear tranformation to the output of each neuron."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Why We need activation](./activation_function/why_need_activation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After each layer, we typically use an activation function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Popular activation functions\r\n",
    "* ### step function\r\n",
    "* ### sigmoid\r\n",
    "* ### tanh\r\n",
    "* ### relu\r\n",
    "* ### leaky relu\r\n",
    "* ### softmax"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step Function <span style = 'color:cyan'>(Not use in practice)</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Step Function](./activation_function/step_function.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sigmoid <span style = 'color:cyan'>(typically in last layer of binary classification)</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Sigmoid](./activation_function/sigmoid.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TanH Function <span style = 'color:cyan'>(hidden layers)</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![TanH function](./activation_function/tanh.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU Function <span style = 'color:cyan'>(if you do not know what to use, just use a relu for hidden layers)</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "relu will output 0 for negative values and return the positive values itself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ReLU Function](./activation_function/relu.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Leaky ReLU Function <span style = 'color:cyan'>(improved version of ReLU. This function solves the vanishing gradient problem)</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "a is typically a really really small value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ReLU Function](./activation_function/leaky_relu.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "whenever you notice your weight did not update more, use leaky relu instead of relu."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax <span style = 'color:cyan'>(good for last layer in multi class classification problem)</span>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}