{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[Read here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving\r\n",
    "* Model\r\n",
    "* State Dictionary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### torch.save(args, PATH) \r\n",
    "### it can be used to save tensor models or <span style='color:cyan'>any dict</span> as parameter for saving.\r\n",
    "torch.save use python <span style='color:cyan'>pickle module</span> to serialize the objects and saves them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\r\n",
    "\r\n",
    "This is the lazy method. The disadvantage is that serialized data is bound to the specific classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.save (model, PATH)\r\n",
    "\r\n",
    "model = torch.load(PATH)\r\n",
    "model.eval()\r\n",
    "\r\n",
    "<span style='color:cyan'>while loading, the model class must be defined somewhere. Pickle does not save the model class itself. Rather, it saves a path to the file containing the class, which is used during load time. Because of this, your code can break in various ways when used in other projects or after refactors.</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## State Dictionary <span style='color:cyan'>(Recommanded Way)</span>\r\n",
    "save only the parameters\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.save(model.state_dict(), PATH)\r\n",
    "\r\n",
    "model = Model(*args,**kwargs)\r\n",
    "\r\n",
    "model.load_state_dict(torch.load(PATH))\r\n",
    "\r\n",
    "model.eval()\r\n",
    "\r\n",
    "<span style='color:cyan'>model must be created again with parameters</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "class Model(nn.Module):\r\n",
    "    def __init__(self, n_input_features):\r\n",
    "        super(Model,self).__init__()\r\n",
    "        self.linear = nn.Linear(in_features=n_input_features, out_features=1)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        y_pred = torch.sigmoid(self.linear(x))\r\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "model = Model(n_input_features=6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving and loading the whole model\r\n",
    "<span style='color:cyan'> file must be ended with \".pth\" or \".pt\".</span>\r\n",
    "\r\n",
    "loading the model from other file in load_model.py<span style='color:cyan'> which will give you error due to window 10</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "FILE = 'whole_model.pt'\r\n",
    "torch.save(model, FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "loaded_model = torch.load(FILE)\r\n",
    "loaded_model.eval()\r\n",
    "\r\n",
    "for param in loaded_model.parameters():\r\n",
    "    print(param)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0268, -0.2908, -0.0573,  0.0077, -0.0466,  0.2516]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0584], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the state_dict\r\n",
    "loading state_dict in load_state_dict.py\r\n",
    "\r\n",
    "we have to <span style='color:cyan'>rebuild the model class.</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "FILE = \"state_dict.pt\"\r\n",
    "torch.save(model.state_dict(),FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "loaded_model = Model(n_input_features=6)\r\n",
    "loaded_model.load_state_dict(torch.load(FILE))\r\n",
    "loaded_model.eval()\r\n",
    "\r\n",
    "for param in loaded_model.parameters():\r\n",
    "    print(param)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0268, -0.2908, -0.0573,  0.0077, -0.0466,  0.2516]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0584], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving Checkpoint during Training\r\n",
    "\r\n",
    "load_check_point will also be in load_state_dict.py"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "learning_rate = 0.01\r\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\r\n",
    "print(optimizer.state_dict())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "checkpoint = {\r\n",
    "    'epoch': 90,\r\n",
    "    'model_state':model.state_dict(),\r\n",
    "    'optim_state':optimizer.state_dict()\r\n",
    "}\r\n",
    "\r\n",
    "torch.save(checkpoint,'checkpoint.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "loaded_checkpoint = torch.load('checkpoint.pt')\r\n",
    "epoch = loaded_checkpoint['epoch']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **we have to recreate and address the model and optimizer**\r\n",
    "<span style='color:cyan'>we do not need the correct learning rate</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "loaded_model = Model(n_input_features=6)\r\n",
    "# different learning rate\r\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "loaded_model.load_state_dict(loaded_checkpoint['model_state'])\r\n",
    "optimizer.load_state_dict(loaded_checkpoint['optim_state'])\r\n",
    "# check that the learning rate is loaded now.\r\n",
    "print(optimizer.state_dict())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving on GPU, Load on CPU\r\n",
    "\r\n",
    "saving"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda')\r\n",
    "model.to(device)\r\n",
    "torch.save(model.state_dict(),PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "device = torch.device('cpu')\r\n",
    "model = Model(*args,**kwargs)\r\n",
    "model.load_state_dict(torch.load(PATH,map_location=device))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3864/2338098532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save on GPU, Load on GPU\r\n",
    "\r\n",
    "saving"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda')\r\n",
    "model.to(device)\r\n",
    "torch.save(model.state_dict(),PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda')\r\n",
    "model = Model(*args,**kwargs)\r\n",
    "model.load_state_dict(torch.load(PATH))\r\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save on CPU, Load on GPU\r\n",
    "\r\n",
    "saving"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.save(model.state_dict(),PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "loading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda')\r\n",
    "model = Model(*args,**kwargs)\r\n",
    "model.load_state_dict(torch.load(PATH,map_location='cuda:0'))\r\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "9264711d228fe35bbbcc49f95b90edfc376310506fc14cf04059be83b6263525"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}