{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **CNN works on images and below is the example of the consisting layers.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./convolutional/convolutional_0.png\" alt=\"convolutional\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./convolutional/convolutional_1.png\" alt=\"convolutional\" />\r\n",
    "<img src=\"./convolutional/convolutional_2.png\" alt=\"convolutional\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## after the convolution layers, there are pooling layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![pooling](./convolutional/max_pool.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![CNN](./convolutional/convolutional_4.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import torch.nn.functional as F "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "#Hyper Parameter\r\n",
    "num_epochs = 10\r\n",
    "batch_size = 4\r\n",
    "learning_rate = 0.001"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset has PILImage images of rgb value [0,255].\r\n",
    "## we have to normalize that rgb values.\r\n",
    "## <span style = 'color:cyan'>To normalize with pytorch, image array needs to be Tensor </span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "transforming = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./dataset',train=True,download=True,transform=transforming)\r\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./dataset',train=False,download=True,transform=transforming)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Shape of the dataset is as follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(f'The shape of training dataset is {train_dataset.data.shape} and that of test dataset is {test_dataset.data.shape}')\r\n",
    "print(f'Type of data is {type(train_dataset.data)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of training dataset is (50000, 32, 32, 3) and that of test dataset is (10000, 32, 32, 3)\n",
      "Type of data is <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRANSFOMRING OF SINGLE IMAGE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "test_img = train_dataset.data[0]\r\n",
    "print(type(test_img))\r\n",
    "print(test_img.dtype)\r\n",
    "print(f'Mean and std before normalize:\\nmean = {test_img.mean()}, std = {test_img.std()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "uint8\n",
      "Mean and std before normalize:\n",
      "mean = 103.447265625, std = 51.97371837166704\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "test_tensor = transforming(test_img)\r\n",
    "print(type(test_tensor))\r\n",
    "print(f'Mean and std after normalize:\\nmean = {test_tensor.mean()}, std = {test_tensor.std()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Mean and std after normalize:\n",
      "mean = -0.18864887952804565, std = 0.4077033996582031\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "#back to pipeline\r\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\r\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style='color:cyan'>kernel_size means filter_size in convolutional layer</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <span style='color:cyan'>Check details of image size reducing down in test_cnn.py</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class ConvNet(nn.Module):\r\n",
    "    def __init__(self,inputs_channel_size,outputs_channel_size,filter_size,pool_kernel_size):\r\n",
    "        super(ConvNet,self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(in_channels = inputs_channel_size,\r\n",
    "                               out_channels = outputs_channel_size, \r\n",
    "                               kernel_size = filter_size)\r\n",
    "        self.pool = nn.MaxPool2d(kernel_size = pool_kernel_size)\r\n",
    "        self.conv2 = nn.Conv2d(in_channels = outputs_channel_size,\r\n",
    "                               out_channels = 16,\r\n",
    "                               kernel_size = filter_size)\r\n",
    "        self.fc1 = nn.Linear(in_features = 16*5*5,\r\n",
    "                             out_features = 120)\r\n",
    "        self.fc2 = nn.Linear(in_features = 120,\r\n",
    "                             out_features = 84)\r\n",
    "        self.fc3 = nn.Linear(in_features = 84,\r\n",
    "                             out_features = 10)    \r\n",
    "    \r\n",
    "    def forward(self,x):\r\n",
    "        x = self.pool(F.relu(self.conv1(x)))\r\n",
    "        x = self.pool(F.relu(self.conv2(x)))\r\n",
    "        x = x.view(-1,16*5*5)  # tensor flatten\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.fc3(x)\r\n",
    "        return x\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style='color:cyan'>The size of image would be smaller after leaving the convlutional layer.</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "calculation of image size after pooling is as below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![conv calculation](./convolutional/output_size_after_pooling.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "original size is (32x32)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "filter AKA kernel size is 5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's why <span style='color:cyan'>32x32 would become 28x28</span> after first convolutional layer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "in_channel_size = 3 # rgb colors channels\r\n",
    "out_channel_size = 6 # anything you want\r\n",
    "filter_size = 5\r\n",
    "pool_kernel_size = (2,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "model = ConvNet(inputs_channel_size=in_channel_size,\r\n",
    "                outputs_channel_size=out_channel_size,pool_kernel_size=pool_kernel_size,\r\n",
    "                filter_size = filter_size).to(device)\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "n_total_steps = len(train_loader)\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for i, (images,labels) in enumerate(train_loader):\r\n",
    "        # watch test_cnn.py  \r\n",
    "        # origin shape: [4,3,32,32] = 4,3,1024\r\n",
    "        images = images.to(device)\r\n",
    "        labels = labels.to(device)\r\n",
    "        #forward pass\r\n",
    "        outputs = model(images)\r\n",
    "        loss = criterion(outputs,labels)\r\n",
    "        #backward\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        if (i+1)%2000 == 0:\r\n",
    "            print(f'Epoch {epoch+1}/{num_epochs},step {i+1}/{n_total_steps}, loss {loss.item():.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10,step 2000/12500, loss 2.362\n",
      "Epoch 1/10,step 4000/12500, loss 2.302\n",
      "Epoch 1/10,step 6000/12500, loss 2.297\n",
      "Epoch 1/10,step 8000/12500, loss 2.276\n",
      "Epoch 1/10,step 10000/12500, loss 2.293\n",
      "Epoch 1/10,step 12000/12500, loss 2.254\n",
      "Epoch 2/10,step 2000/12500, loss 1.790\n",
      "Epoch 2/10,step 4000/12500, loss 1.713\n",
      "Epoch 2/10,step 6000/12500, loss 1.931\n",
      "Epoch 2/10,step 8000/12500, loss 1.333\n",
      "Epoch 2/10,step 10000/12500, loss 1.828\n",
      "Epoch 2/10,step 12000/12500, loss 1.701\n",
      "Epoch 3/10,step 2000/12500, loss 1.985\n",
      "Epoch 3/10,step 4000/12500, loss 1.139\n",
      "Epoch 3/10,step 6000/12500, loss 0.691\n",
      "Epoch 3/10,step 8000/12500, loss 2.052\n",
      "Epoch 3/10,step 10000/12500, loss 1.847\n",
      "Epoch 3/10,step 12000/12500, loss 1.286\n",
      "Epoch 4/10,step 2000/12500, loss 1.077\n",
      "Epoch 4/10,step 4000/12500, loss 2.243\n",
      "Epoch 4/10,step 6000/12500, loss 0.777\n",
      "Epoch 4/10,step 8000/12500, loss 1.768\n",
      "Epoch 4/10,step 10000/12500, loss 1.585\n",
      "Epoch 4/10,step 12000/12500, loss 1.576\n",
      "Epoch 5/10,step 2000/12500, loss 1.084\n",
      "Epoch 5/10,step 4000/12500, loss 0.812\n",
      "Epoch 5/10,step 6000/12500, loss 1.037\n",
      "Epoch 5/10,step 8000/12500, loss 1.045\n",
      "Epoch 5/10,step 10000/12500, loss 1.444\n",
      "Epoch 5/10,step 12000/12500, loss 1.142\n",
      "Epoch 6/10,step 2000/12500, loss 0.963\n",
      "Epoch 6/10,step 4000/12500, loss 1.192\n",
      "Epoch 6/10,step 6000/12500, loss 1.352\n",
      "Epoch 6/10,step 8000/12500, loss 2.003\n",
      "Epoch 6/10,step 10000/12500, loss 1.060\n",
      "Epoch 6/10,step 12000/12500, loss 1.401\n",
      "Epoch 7/10,step 2000/12500, loss 1.381\n",
      "Epoch 7/10,step 4000/12500, loss 0.958\n",
      "Epoch 7/10,step 6000/12500, loss 1.432\n",
      "Epoch 7/10,step 8000/12500, loss 1.375\n",
      "Epoch 7/10,step 10000/12500, loss 0.926\n",
      "Epoch 7/10,step 12000/12500, loss 1.057\n",
      "Epoch 8/10,step 2000/12500, loss 0.837\n",
      "Epoch 8/10,step 4000/12500, loss 1.614\n",
      "Epoch 8/10,step 6000/12500, loss 1.058\n",
      "Epoch 8/10,step 8000/12500, loss 1.186\n",
      "Epoch 8/10,step 10000/12500, loss 1.139\n",
      "Epoch 8/10,step 12000/12500, loss 1.112\n",
      "Epoch 9/10,step 2000/12500, loss 0.863\n",
      "Epoch 9/10,step 4000/12500, loss 1.769\n",
      "Epoch 9/10,step 6000/12500, loss 1.568\n",
      "Epoch 9/10,step 8000/12500, loss 1.868\n",
      "Epoch 9/10,step 10000/12500, loss 1.850\n",
      "Epoch 9/10,step 12000/12500, loss 1.551\n",
      "Epoch 10/10,step 2000/12500, loss 1.854\n",
      "Epoch 10/10,step 4000/12500, loss 1.708\n",
      "Epoch 10/10,step 6000/12500, loss 0.788\n",
      "Epoch 10/10,step 8000/12500, loss 1.387\n",
      "Epoch 10/10,step 10000/12500, loss 1.272\n",
      "Epoch 10/10,step 12000/12500, loss 0.562\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "with torch.no_grad():\r\n",
    "    n_correct = 0\r\n",
    "    n_sample = 0\r\n",
    "    n_class_correct = [0 for i in range(10)]\r\n",
    "    n_class_samples = [0 for i in range(10)]\r\n",
    "    for images,labels in test_loader:\r\n",
    "        images = images.to(device)\r\n",
    "        labels = labels.to(device)\r\n",
    "        outputs = model(images)\r\n",
    "        # max returns (value,index)\r\n",
    "        _,predicted_index = torch.max(outputs,1)\r\n",
    "        n_sample += labels.size(0)\r\n",
    "        n_correct += (predicted_index==labels).sum().item()\r\n",
    "        \r\n",
    "        for i in range(batch_size):\r\n",
    "            label = labels[i]\r\n",
    "            pred = predicted_index[i]\r\n",
    "            if (label == pred):\r\n",
    "                n_class_correct[label] += 1\r\n",
    "            n_class_samples[label] += 1\r\n",
    "        \r\n",
    "    acc = 100.0 * n_correct/n_sample\r\n",
    "    print(f'Accuracy is {acc}')\r\n",
    "    \r\n",
    "    for i in range(10):\r\n",
    "        acc = 100.0 * n_class_correct[i]/n_class_samples[i]\r\n",
    "        print(f'Accuracy of {classes[i]}:{acc}')\r\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy is 56.58\n",
      "Accuracy of plane:59.8\n",
      "Accuracy of car:68.4\n",
      "Accuracy of bird:41.5\n",
      "Accuracy of cat:33.8\n",
      "Accuracy of deer:35.8\n",
      "Accuracy of dog:45.1\n",
      "Accuracy of frog:75.9\n",
      "Accuracy of horse:68.6\n",
      "Accuracy of ship:75.8\n",
      "Accuracy of truck:61.1\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "9264711d228fe35bbbcc49f95b90edfc376310506fc14cf04059be83b6263525"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}