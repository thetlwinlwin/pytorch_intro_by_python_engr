{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![softmax_equation](./softmax_and_cross_entropy/softmax_equation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![softmax_equation](./softmax_and_cross_entropy/softmax_layer.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "sum of outputs after softmax layer is is 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# this is just a mere implementation. We do not use our own function.\r\n",
    "def softmax(x):\r\n",
    "    #axis = 0 is column\r\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\r\n",
    "\r\n",
    "x = np.array([[2],[1],[0.5]])\r\n",
    "print(f'shape is {x.shape} and dimension is {x.ndim}')\r\n",
    "print(f'softmax numpy: {softmax(x)}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape is (3, 1) and dimension is 2\n",
      "softmax numpy: [[0.62853172]\n",
      " [0.2312239 ]\n",
      " [0.14024438]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WITH PYTORCH BUILT-IN SOFTMAX"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style = 'color:cyan'>So the idea is to put more deep-learning-oriented functions in torch.nn.functional and keep general-purpose functions in under torch directly. softmax was deemed to fall into the former, sigmoid in the latter category.While there is torch.softmax, this is by accident (which is why it is not documented). [click here](https://discuss.pytorch.org/t/why-there-isnt-a-method-named-torch-softmax/90554)</span>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x = torch.tensor([[2],[1],[0.5]])\r\n",
    "print(f'dimension is {x.dim()}, size is {x.size()}')\r\n",
    "outputs = nn.functional.softmax(x,dim = 0)\r\n",
    "print(outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dimension is 2, size is torch.Size([3, 1])\n",
      "tensor([[0.6285],\n",
      "        [0.2312],\n",
      "        [0.1402]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![cross_entropy_equation](./softmax_and_cross_entropy/cross_entropy_equation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One-Hot Encoding Vs Label Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![One_Hot vs Label](./softmax_and_cross_entropy/label_vs_one-hot.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <span style = 'color:cyan'>For more information</span> [here](https://www.kaggle.com/alexisbcook/categorical-variables)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def cross_entropy(actual,predicted):\r\n",
    "    loss = -np.sum(actual * np.log(predicted))\r\n",
    "    return loss   #/float(predicted.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# y must be one hot encoded\r\n",
    "# if class 0 : [1 0 0]\r\n",
    "# if class 1 : [0 1 0]\r\n",
    "# if class 2 : [0 0 1]\r\n",
    "Y = np.array([[1],[0],[0]])\r\n",
    "print(f'shape is {Y.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape is (3, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# y_pred has probabilities\r\n",
    "Y_pred_good = np.array([[0.8],[0.05],[0.15]])\r\n",
    "Y_pred_bad = np.array([[0.1],[0.3],[0.6]])\r\n",
    "l1 = cross_entropy(Y,Y_pred_good)\r\n",
    "l2 = cross_entropy(Y,Y_pred_bad)\r\n",
    "print(f'Loss on good prediction is {l1:.3f}')\r\n",
    "print(f'Loss on bad prediction is {l2:.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss on good prediction is 0.223\n",
      "Loss on bad prediction is 2.303\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WITH PYTORCH BUILT-IN CROSS ENTROPY"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **<span style = 'color:cyan'>nn.CrossEntropyLoss applies nn.LogSoftmax + nn.NLLLoss(negative log likelihood loss)</span>**\r\n",
    "\r\n",
    "## <span style = 'color:cyan'>We do not need Softmax in last layer</span>\r\n",
    "\r\n",
    "## label <span style = 'color:cyan'>must not be</span> one-hot encoded\r\n",
    "\r\n",
    "## Y_pred has<span style = 'color:cyan'> raw scores</span>\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "loss = nn.CrossEntropyLoss()\r\n",
    "# 3 samples\r\n",
    "Y = torch.tensor([2,0,1])\r\n",
    "print(Y.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# [2.0,1,0.1] are raw values. Not softmax\r\n",
    "# n_samples x n_classes  = 1 sample is testing with 3 possible classes.\r\n",
    "Y_pred_good = torch.tensor([[0.7,2,3],[2.0,1,0.1],[1,7,4]])\r\n",
    "Y_pred_bad = torch.tensor([[0.5,2.1,0.3],[0.7,2,3],[9,0.9,5]])\r\n",
    "print(f'shape is {Y_pred_bad.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape is torch.Size([3, 3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "l1 = loss(Y_pred_good,Y)\r\n",
    "l2 = loss(Y_pred_bad,Y)\r\n",
    "print(f'Loss on good prediction is {l1:.3f}')\r\n",
    "print(f'Loss on bad prediction is {l2:.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss on good prediction is 0.284\n",
      "Loss on bad prediction is 4.305\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "_, prediction1 = torch.max(Y_pred_good,dim = 1)\r\n",
    "_, prediction2 = torch.max(Y_pred_bad, dim = 1)\r\n",
    "print(prediction1)\r\n",
    "print(prediction2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2, 0, 1])\n",
      "tensor([1, 2, 0])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How we get tensor([2, 0, 1])?\r\n",
    "Y_pred_good is [ [ 0.7, 2, 3 ], [ 2.0, 1, 0.1 ], [ 1, 7, 4 ] ] and\r\n",
    "dim = 1 is declared in the torch.max function. So, max function will look each row in Y_pred_good tensor.\r\n",
    "* the first row [ 0.7, 2, 3 ] has max value at the index of 2.\r\n",
    "* the second row [ 2.0, 1, 0.1 ] has max value at the index of 0.\r\n",
    "* the third row [ 1, 7, 4 ] has max value at the index of 1.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiclass problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class NeuralNet2(nn.Module):\r\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\r\n",
    "        super(NeuralNet2,self).__init__()\r\n",
    "        self.linear1 = nn.Linear(input_size,hidden_size)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.linear2 = nn.Linear(hidden_size,num_classes)\r\n",
    "        \r\n",
    "    def forward(self,x):\r\n",
    "        out = self.linear1(x)\r\n",
    "        out = self.relu(out)\r\n",
    "        #no softmax at the end\r\n",
    "        out = self.linear2(out)\r\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\r\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax built-in"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary Classification problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![binary classification problem](./softmax_and_cross_entropy/binary_classification_problem.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class NeuralNet1(nn.Module):\r\n",
    "    #no_of_class is always one in binary classification problem.\r\n",
    "    def __init__(self,input_size,hidden_size):\r\n",
    "        super(NeuralNet1,self).__init__()\r\n",
    "        self.linear1 = nn.Linear(input_size,hidden_size)\r\n",
    "        self.relu = nn.ReLU()\r\n",
    "        self.linear2 = nn.Linear(hidden_size,1)\r\n",
    "        \r\n",
    "    def forward(self,x):\r\n",
    "        out = self.linear1(x)\r\n",
    "        out = self.relu(out)\r\n",
    "        out = self.linear2(out)\r\n",
    "        y_pred = torch.sigmoid(out)\r\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\r\n",
    "criterion = nn.BCELoss()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "9264711d228fe35bbbcc49f95b90edfc376310506fc14cf04059be83b6263525"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}